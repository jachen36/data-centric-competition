{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a5b82-e78e-4909-a2f5-c0cb808609df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fd4a8-4477-4464-8d73-d51340d44f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from local_lib import utils, train\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3fd72b-e9cd-4613-a26e-84f7d52c33ca",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "See the result of simple model runs and how it relates with the private leader board. It is important to see how closely the local validation set actually match with the test set because I can test more iteration than the restrictive 5 times a week competition rule. \n",
    "\n",
    "# Method\n",
    "\n",
    "1. Repeat the baseline model\n",
    "1. Just run the model with clean dataset. \n",
    "1. Train with nonsensical images removed. \n",
    "1. Train with nonsensical images removed and upsample to 10,000. \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The default validation set from the competition isn't representative of the private leaderboard dataset. This is based on the fact that my local validation accuracy is much higher (0.05+) than the score I got from the leaderboard. It looks like there are more challenging images in the private dataset. \n",
    "\n",
    "Removing noisy images that even human cannot make sense of and data augmentation helped the model generalized better by having a higher validation score comapred to baseline. \n",
    "\n",
    "\n",
    "# Future\n",
    "\n",
    "* Reshuffle the train and validation set to obtain a validation set where the local accuracy and the private dataset matches. \n",
    "* Using optuna to select an optimized image augmentation pipeline. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2adaf-1295-477d-86d1-de989b1033b2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dde7a1-cbb6-450e-88e6-48b05b020b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pt = \"../data/processed/labels_v0.1.0.csv\"\n",
    "source_image_dir = Path('../data/processed/labeling/images')\n",
    "train_image_dir = Path('../data/processed/tmp_training')\n",
    "\n",
    "labels = pd.read_csv(labels_pt)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94f6a4-39f0-4955-88b1-587403e9749b",
   "metadata": {},
   "source": [
    "# Baseline Training\n",
    "\n",
    "The official baseline performance on the public leader board with the original data is `0.65521`.\n",
    "\n",
    "We can see that the validation and the leader board metric are very close to each other which suggestion that there is some variation between my local and cloud env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501f445-f024-457d-bb71-fd5445271571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if train_image_dir.exists():\n",
    "    shutil.rmtree(train_image_dir)\n",
    "\n",
    "_ = utils.copy_group_to_dir(\n",
    "    df=labels, \n",
    "    source_dir=source_image_dir, \n",
    "    dest_dir=train_image_dir, \n",
    "    group= [\"org_source\", \"org_symbol\"]\n",
    "   )\n",
    "assert len(_) == 0\n",
    "\n",
    "train.train_model(train_image_dir, \"../data/raw/label_book\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830524f-1d11-4f00-990d-9523a5f19004",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 100/100\n",
    "259/259 [==============================] - 35s 134ms/step - loss: 0.0242 - accuracy: 0.9942 - val_loss: 1.6930 - val_accuracy: 0.6458\n",
    "102/102 [==============================] - 3s 25ms/step - loss: 1.3684 - accuracy: 0.6814\n",
    "7/7 [==============================] - 0s 31ms/step - loss: 2.7221 - accuracy: 0.5577\n",
    "final loss 1.3684338331222534, final acc 0.6814268231391907\n",
    "test loss 2.722132444381714, test acc 0.557692289352417\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf60599-c8cc-424a-84e4-df180b042502",
   "metadata": {},
   "source": [
    "# Basic data cleaning\n",
    "\n",
    "Relabled the whole dataset which hopefully is a much cleaner dataset. Only `18` out of `813` images were corrected, so the data so the validation was relatively clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6074db-9a9c-4f28-9590-dd626af1cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_val = labels.query(\"org_source == 'val' and org_symbol != symbol\")\n",
    "clean_val.shape\n",
    "\n",
    "if train_image_dir.exists():\n",
    "    shutil.rmtree(train_image_dir)\n",
    "\n",
    "_ = utils.copy_group_to_dir(\n",
    "    df=labels, \n",
    "    source_dir=source_image_dir, \n",
    "    dest_dir=train_image_dir, \n",
    "    group= [\"org_source\", \"symbol\"]\n",
    "   )\n",
    "assert len(_) == 0\n",
    "\n",
    "train.train_model(train_image_dir, \"../data/raw/label_book\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91fc56-a529-4c30-800e-2dc39ab12179",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 100/100\n",
    "259/259 [==============================] - 40s 152ms/step - loss: 0.0200 - accuracy: 0.9961 - val_loss: 1.5971 - val_accuracy: 0.6507\n",
    "102/102 [==============================] - 2s 22ms/step - loss: 1.1378 - accuracy: 0.7171\n",
    "7/7 [==============================] - 0s 19ms/step - loss: 2.2028 - accuracy: 0.6346\n",
    "final loss 1.1377620697021484, final acc 0.7170971632003784\n",
    "test loss 2.202808141708374, test acc 0.6346153616905212\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9414c-bdea-4fb5-9fd7-77c4e7c33aa0",
   "metadata": {},
   "source": [
    "## Train with nonsensical image removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789da97-0596-4e91-b73e-a327da2669af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_filtered = labels.query(\"suggestion != 'remove' or org_source == 'val'\")\n",
    "\n",
    "if train_image_dir.exists():\n",
    "    shutil.rmtree(train_image_dir)\n",
    "\n",
    "_ = utils.copy_group_to_dir(\n",
    "    df=labels_filtered, \n",
    "    source_dir=source_image_dir, \n",
    "    dest_dir=train_image_dir, \n",
    "    group= [\"org_source\", \"symbol\"]\n",
    "   )\n",
    "assert len(_) == labels.query(\"suggestion == 'remove' and org_source == 'train'\").shape[0]\n",
    "\n",
    "train.train_model(train_image_dir, \"../data/raw/label_book\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb295d-84c2-40a1-81f0-dd6af93767a6",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 100/100\n",
    "249/249 [==============================] - 36s 145ms/step - loss: 0.0172 - accuracy: 0.9955 - val_loss: 1.4800 - val_accuracy: 0.6851\n",
    "102/102 [==============================] - 3s 27ms/step - loss: 1.1938 - accuracy: 0.7319\n",
    "7/7 [==============================] - 0s 23ms/step - loss: 1.8671 - accuracy: 0.5577\n",
    "final loss 1.1938356161117554, final acc 0.7318572998046875\n",
    "test loss 1.8670954704284668, test acc 0.557692289352417\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15ff36-06c0-4905-b024-02f2f01516f8",
   "metadata": {},
   "source": [
    "# Simple augmentations and upsampling up to 10,000 images\n",
    "\n",
    "10,000 is the maximum amount of images allow for the competition. This is the sum of train and validation. \n",
    "\n",
    "nonsensical images were removed before augmentation because augmenting them will provide little or detrimental results to the training since they are noise. \n",
    "\n",
    "The overall performance when from `0.65521` to `0.75000`! However, the validation set metrics has wider gap from the leaderboard than the baseline model. It could be because I am using the \"cleaned\" dataset, but the actual change is relatively small and won't impact the metrics that much. Further investgiation is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3b5ac-3692-4ebb-b953-03471965a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 10_000 - 8\n",
    "remainder = max_images - len(labels)\n",
    "\n",
    "if train_image_dir.exists():\n",
    "    shutil.rmtree(train_image_dir)\n",
    "\n",
    "_ = utils.copy_group_to_dir(\n",
    "    df=labels, \n",
    "    source_dir=source_image_dir, \n",
    "    dest_dir=train_image_dir, \n",
    "    group= [\"org_source\", \"symbol\"]\n",
    "   )\n",
    "assert len(_) == 0\n",
    "\n",
    "labels_filtered = labels.query(\"suggestion != 'remove' and org_source != 'val'\")\n",
    "images_2_aug = labels_filtered.query(\"confusing == 'none' and org_source != 'val'\").copy()\n",
    "\n",
    "images_2_aug[\"aug_amount\"] = 0\n",
    "\n",
    "images_per_group = remainder // 10\n",
    "\n",
    "symbol_counts = images_2_aug.symbol.value_counts()\n",
    "\n",
    "# balance sampling so we don't bias toward common symbols\n",
    "for row in symbol_counts.to_frame().reset_index().itertuples():\n",
    "    \n",
    "    symbol_sample_min = int(images_per_group/row.symbol)\n",
    "    sample_amount = [symbol_sample_min] * row.symbol\n",
    "    sample_remainder = images_per_group - sum(sample_amount)\n",
    "    if sample_remainder > 0:\n",
    "        sample_amount[:sample_remainder] = [symbol_sample_min + 1] * sample_remainder\n",
    "    random.shuffle(sample_amount)\n",
    "    images_2_aug.loc[images_2_aug.symbol == row.index, \"aug_amount\"] = sample_amount\n",
    "\n",
    "assert images_2_aug.aug_amount.sum() <= remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d29ec-eabc-4239-9030-dfa5e02be50c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Crop(px=(1, 16), keep_size=True),\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.GaussianBlur(sigma=(0, 3.0))\n",
    "])\n",
    "\n",
    "utils.apply_aug(images_2_aug, source_image_dir, train_image_dir, seq, [\"org_source\", \"symbol\"])\n",
    "\n",
    "train.train_model(train_image_dir, \"../data/raw/label_book\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c1572-784b-4692-8d00-2d1bf13f85ed",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 100/100\n",
    "1148/1148 [==============================] - 147s 128ms/step - loss: 0.0220 - accuracy: 0.9926 - val_loss: 1.3617 - val_accuracy: 0.7306\n",
    "102/102 [==============================] - 2s 21ms/step - loss: 0.9264 - accuracy: 0.8118\n",
    "7/7 [==============================] - 0s 29ms/step - loss: 1.7896 - accuracy: 0.7115\n",
    "final loss 0.9263656139373779, final acc 0.8118081092834473\n",
    "test loss 1.7895989418029785, test acc 0.7115384340286255\n",
    "```\n",
    "\n",
    "Leader board result  \n",
    "\n",
    "Aug 12 - submission-01 = 0.75000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
